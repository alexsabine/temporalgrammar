\documentclass[11pt,a4paper]{article}

% Packages
\usepackage{amsmath,amsthm,amssymb,amsfonts}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{thmtools}
\usepackage{booktabs}
\usepackage{array}
\usepackage[margin=1in]{geometry}
\usepackage{xcolor}
\usepackage{tcolorbox}

% Theorem environments
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}

\theoremstyle{remark}
\newtheorem*{notation}{Notation}
\newtheorem*{claim}{Claim}

% Custom commands
\newcommand{\E}{\mathbb{E}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Sur}{\mathcal{S}}  % Surprisal/cost
\newcommand{\M}{\mathcal{M}}
\newcommand{\Sol}{\mathcal{M}_{\text{Sol}}}
\newcommand{\Kol}{K}

% Title
\title{\textbf{Coherence-Rupture-Regeneration and Solomonoff Induction:\\
A Mathematical Comparison}}
\author{Mathematical Analysis}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This document presents a mathematical analysis comparing the Coherence-Rupture-Regeneration (CRR) framework with Solomonoff induction. We establish correspondence theorems between the two frameworks and identify what each provides. The main findings are: (1) CRR surprisal corresponds to accumulated conditional Kolmogorov complexity; (2) CRR rupture implements MDL model switching; (3) CRR regeneration implements a soft MDL evidence-weighting mechanism related to, but distinct from, Solomonoff's universal prior; (4) the frameworks are compatible, with CRR providing operational details that Solomonoff leaves unspecified. We carefully distinguish the pairwise switching threshold from the global temperature parameter.
\end{abstract}

\tableofcontents
\newpage

%==============================================================================
\section{Introduction}
%==============================================================================

\subsection{Scope}

This document compares two frameworks for inductive inference:

\begin{enumerate}[label=(\roman*)]
    \item \textbf{Solomonoff Induction} (1964): A theoretical framework for sequence prediction based on algorithmic information theory and Kolmogorov complexity. It provides optimality guarantees but is incomputable.

    \item \textbf{Coherence-Rupture-Regeneration (CRR)}: A framework describing how bounded systems maintain identity through discontinuous change, grounded in Bayesian model comparison.
\end{enumerate}

We address three questions:
\begin{enumerate}
    \item What is the mathematical relationship between CRR and Solomonoff induction?
    \item Are the two frameworks compatible?
    \item What does each framework provide that the other does not?
\end{enumerate}

\subsection{Summary of Findings}

\begin{tcolorbox}[colback=gray!5,colframe=gray!75!black,title=Main Findings]
\textbf{What Solomonoff Induction Provides:}
\begin{itemize}
    \item The optimal prior over hypotheses: $P(h) = 2^{-K(h)}$
    \item The optimal posterior: $P(h|y) \propto P(y|h) \cdot 2^{-K(h)}$
    \item Proof of optimality (dominates all computable predictors)
\end{itemize}

\textbf{What Solomonoff Does Not Provide:}
\begin{itemize}
    \item When to act on accumulated evidence
    \item A computable implementation
    \item The mechanism of model change
    \item Tunable parameters for different contexts
\end{itemize}

\textbf{What CRR Adds:}
\begin{itemize}
    \item Decision rule: switch when $\Sur_m - \Sur_{m'} > \Delta_{m,m'}$
    \item Computable approximation via temporal surprisal accumulation
    \item Regeneration operator with temperature parameter $\Omega$
    \item Separation of switching threshold $\Delta$ from selection sharpness $\Omega$
\end{itemize}
\end{tcolorbox}

\subsection{The Three-Framework Synthesis}

\begin{center}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Framework} & \textbf{Specifies} & \textbf{Mathematical Form} \\
\midrule
Solomonoff & What is optimal & $P(h) = 2^{-K(h)}$ \\
Free Energy Principle & What to minimize & $F = \E[\log q] - \E[\log p]$ \\
CRR & When and how & $\Sur \to \delta \to R$ with threshold $\Delta$, temperature $\Omega$ \\
\bottomrule
\end{tabular}
\end{center}

%==============================================================================
\section{Preliminaries}
%==============================================================================

\subsection{Algorithmic Information Theory}

\begin{definition}[Kolmogorov Complexity]
The \emph{Kolmogorov complexity} of a string $x$ with respect to universal machine $U$ is:
\begin{equation}
K_U(x) = \min\{|p| : U(p) = x\}
\end{equation}
where $|p|$ denotes the length of program $p$ in bits.
\end{definition}

\begin{definition}[Conditional Kolmogorov Complexity]
The complexity of $x$ given $y$ is:
\begin{equation}
K(x|y) = \min\{|p| : U(p, y) = x\}
\end{equation}
\end{definition}

\begin{theorem}[Invariance Theorem]
For any two universal machines $U_1, U_2$, there exists a constant $c$ such that for all $x$:
\begin{equation}
|K_{U_1}(x) - K_{U_2}(x)| \leq c
\end{equation}
\end{theorem}

\subsection{Solomonoff Induction}

\begin{definition}[Solomonoff Prior]
The \emph{Solomonoff prior} over hypotheses (programs) is:
\begin{equation}
P_{\text{Sol}}(h) = 2^{-K(h)}
\end{equation}
This assigns higher probability to simpler hypotheses.
\end{definition}

\begin{definition}[Solomonoff Posterior]
Given data $y$, the posterior over hypotheses is:
\begin{equation}
P_{\text{Sol}}(h | y) \propto P(y | h) \cdot 2^{-K(h)} = 2^{-K(y|h)} \cdot 2^{-K(h)} \approx 2^{-K(h,y)}
\end{equation}
where the approximation uses the chain rule $K(h,y) = K(h) + K(y|h) + O(\log n)$.
\end{definition}

\begin{theorem}[Solomonoff Convergence]
Let $\mu$ be any computable measure. The expected total squared prediction error is bounded:
\begin{equation}
\E_\mu\left[\sum_{n=1}^{\infty} (P_{\text{Sol}}(x_{n+1}|x_{1:n}) - \mu(x_{n+1}|x_{1:n}))^2\right] \leq K(\mu) \ln 2
\end{equation}
\end{theorem}

\begin{remark}[Incomputability]
Solomonoff induction is incomputable because computing $K(x)$ requires solving the halting problem.
\end{remark}

\subsection{CRR Framework}

\begin{definition}[CRR System]
A \emph{CRR system} is a tuple $(\M, Y, \Pi, \Delta, \Omega, \Sur, R)$ where:
\begin{itemize}
    \item $\M = \{m, m', \ldots\}$ is a set of generative models
    \item $Y = \R^d$ is the observation space
    \item $\Pi: \M \to \text{PD}(d)$ assigns precision matrices
    \item $\Delta: \M \times \M \to \R$ is the \textbf{switching threshold} (pairwise, model-dependent)
    \item $\Omega > 0$ is the \textbf{temperature/rigidity parameter} (global)
    \item $\Sur: \M \times \N \to \R_+$ is the surprisal accumulator
    \item $R$ is the regeneration operator
\end{itemize}
\end{definition}

\begin{remark}[Two Distinct Parameters]
We explicitly separate:
\begin{itemize}
    \item $\Delta_{m,m'}$: The threshold for switching from model $m$ to $m'$, related to description-length overhead and prior odds
    \item $\Omega$: A temperature-like parameter controlling how sharply regeneration weights historical states
\end{itemize}
These serve different functions and should not be conflated.
\end{remark}

\begin{definition}[CRR Operators]\label{def:crr-operators}
The three CRR operators are:

\textbf{Surprisal (Cost) Accumulation:}
\begin{equation}\label{eq:surprisal}
\Sur_m(n) = \frac{1}{2}\sum_{i=1}^{n} (y_i - g_m(\mu_i))^\top \Pi_m (y_i - g_m(\mu_i))
\end{equation}
This is a \emph{cost}: higher values indicate worse model fit.

\textbf{Rupture Condition:}
\begin{equation}\label{eq:rupture}
\text{Switch } m \to m' \quad \text{when} \quad \Sur_m(n) - \Sur_{m'}(n) > \Delta_{m,m'}
\end{equation}

\textbf{Regeneration:}
\begin{equation}\label{eq:regeneration}
R[\phi](t) = \frac{1}{Z}\int_0^t \phi(\tau) \cdot \exp\left(-\frac{\Sur(\tau)}{\Omega}\right) \cdot \Theta(t-\tau) \, d\tau
\end{equation}
Note the \textbf{negative sign}: states with \emph{lower} surprisal (better fit) receive \emph{higher} weight.
\end{definition}

\begin{remark}[Sign Convention]
The negative sign in the regeneration exponent is essential for consistency:
\begin{itemize}
    \item $\Sur$ is a cost (higher = worse fit, analogous to $-\log P$ or $K$)
    \item $\exp(-\Sur/\Omega)$ gives higher weight to lower-cost states
    \item This aligns with Boltzmann weighting $\exp(-E/kT)$ where low energy is favored
\end{itemize}
\end{remark}

%==============================================================================
\section{Correspondence Theorems}
%==============================================================================

\subsection{Surprisal and Kolmogorov Complexity}

\begin{theorem}[Surprisal-Complexity Correspondence]\label{thm:surprisal-complexity}
Let $y_{1:n}$ be an observation sequence and $m$ a computable generative model. Define the \emph{algorithmic surprisal}:
\begin{equation}
\Sur^{\text{alg}}_m(n) = \sum_{i=1}^{n} K(y_i | y_{<i}, m)
\end{equation}
Then for CRR surprisal under Gaussian observations:
\begin{equation}
\Sur_m(n) = \Sur^{\text{alg}}_m(n) + O(n)
\end{equation}
where the $O(n)$ term accounts for encoding overhead.
\end{theorem}

\begin{proof}
\textbf{Step 1:} By Levin's coding theorem, for any computable distribution $P$:
\begin{equation}
-\log P(x) = K(x) + O(K(P))
\end{equation}

\textbf{Step 2:} CRR surprisal under Gaussian observations equals negative log-likelihood:
\begin{equation}
\Sur_m(n) = -\log p(y_{1:n} | m) + \frac{n}{2}\log\det(2\pi\Sigma_m)
\end{equation}

\textbf{Step 3:} Combining:
\begin{align}
\Sur_m(n) &= \sum_{i=1}^{n} [-\log p(y_i | y_{<i}, m)] + O(n) \\
&= \sum_{i=1}^{n} [K(y_i | y_{<i}, m) + O(1)] + O(n) = \Sur^{\text{alg}}_m(n) + O(n)
\end{align}
\end{proof}

\begin{remark}[Temporal Survival as Complexity Proxy]
Models with low surprisal accumulation over time \emph{behave as if} they have low Kolmogorov complexity, without computing $K$ directly. This provides a computable proxy for the incomputable complexity measure.
\end{remark}

\subsection{Rupture and Minimum Description Length}

\begin{theorem}[Rupture as MDL Model Switching]\label{thm:rupture-mdl}
The CRR rupture condition implements MDL model switching. Specifically, the switching threshold corresponds to:
\begin{equation}
\Delta_{m,m'} = K(m') - K(m) + K(\text{switch})
\end{equation}
which is the description-length overhead for adopting model $m'$ over $m$.
\end{theorem}

\begin{proof}
The MDL principle selects the model minimizing total description length:
\begin{equation}
m^* = \arg\min_m [K(m) + K(y_{1:n} | m)]
\end{equation}

Model $m'$ is preferred over $m$ when:
\begin{equation}
K(m') + K(y_{1:n} | m') < K(m) + K(y_{1:n} | m)
\end{equation}

Rearranging:
\begin{equation}
K(y_{1:n} | m) - K(y_{1:n} | m') > K(m') - K(m)
\end{equation}

Substituting CRR variables via Theorem \ref{thm:surprisal-complexity}:
\begin{equation}
\Sur_m(n) - \Sur_{m'}(n) > \underbrace{K(m') - K(m)}_{\Delta_{m,m'}}
\end{equation}
\end{proof}

\begin{remark}[Threshold is Model-Dependent]
The switching threshold $\Delta_{m,m'}$ depends on the complexity difference between models. This is \emph{not} the same as the temperature parameter $\Omega$.
\end{remark}

\subsection{Regeneration and Evidence Weighting}

\begin{theorem}[Regeneration as MDL Evidence Weighting]\label{thm:regeneration-mdl}
CRR regeneration implements a soft MDL evidence-weighting mechanism. For a hypothesis $h$ evaluated on data $y$:
\begin{equation}
w(h; y) \propto \exp\left(-\frac{\Sur_h(y)}{\Omega}\right) \approx \exp\left(-\frac{K(y|h)}{\Omega \ln 2}\right) = 2^{-K(y|h)/\Omega'}
\end{equation}
where $\Omega' = \Omega \ln 2$.

This weights hypotheses by how well they compress the data (the \textbf{likelihood/evidence term}), not by the prior complexity of the hypothesis itself.
\end{theorem}

\begin{proof}
From Definition \ref{def:crr-operators}, regeneration weights by $\exp(-\Sur/\Omega)$.

By Theorem \ref{thm:surprisal-complexity}:
\begin{equation}
\Sur_h(y) \approx K(y|h) + O(n)
\end{equation}

Therefore:
\begin{equation}
w(h; y) \propto \exp\left(-\frac{K(y|h) + O(n)}{\Omega}\right) \propto 2^{-K(y|h)/(\Omega \ln 2)}
\end{equation}
\end{proof}

\begin{remark}[Distinction from Solomonoff Prior]\label{rem:distinction}
This is \textbf{not} the Solomonoff prior $2^{-K(h)}$ over hypotheses. The correspondence is:
\begin{center}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Solomonoff} & \textbf{CRR Regeneration} \\
\midrule
Prior: $P(h) \propto 2^{-K(h)}$ & Not directly represented \\
Likelihood: $P(y|h) \propto 2^{-K(y|h)}$ & $w(h;y) \propto \exp(-\Sur_h(y)/\Omega)$ \\
Posterior: $P(h|y) \propto 2^{-K(h,y)}$ & Would require adding prior term \\
\bottomrule
\end{tabular}
\end{center}

CRR regeneration captures the \textbf{evidence/likelihood} component of Bayesian inference, weighting hypotheses by how well they explain the data. To recover the full Solomonoff posterior, one would need to additionally weight by a prior term proportional to $2^{-K(h)}$ or equivalently $\exp(-K(h)/\Omega')$.
\end{remark}

\begin{corollary}[Full Posterior Recovery]
To recover Solomonoff-like posterior weighting, CRR would need:
\begin{equation}
w_{\text{full}}(h; y) \propto \underbrace{\exp\left(-\frac{K(h)}{\Omega'}\right)}_{\text{prior term}} \cdot \underbrace{\exp\left(-\frac{K(y|h)}{\Omega'}\right)}_{\text{CRR regeneration}} = \exp\left(-\frac{K(h,y)}{\Omega'}\right)
\end{equation}
where the prior term could be implemented via the model prior $p(m)$ in the CRR system.
\end{corollary}

\subsection{Parameter Identification}

\begin{proposition}[Temperature-Complexity Scaling]\label{prop:temperature}
For CRR regeneration weights to match Solomonoff likelihood weighting exactly:
\begin{equation}
\exp\left(-\frac{\Sur}{\Omega}\right) = 2^{-K(y|h)} \quad \Rightarrow \quad \Omega = \frac{1}{\ln 2} \approx 1.443 \text{ (in nats)}
\end{equation}
\end{proposition}

\begin{remark}[The $\Omega = 1/\pi$ Conjecture]
CRR documentation conjectures a universal value $\Omega = 1/\pi \approx 0.318$. This is \textbf{not} the same as the Solomonoff-matching value $1/\ln 2 \approx 1.443$.

If $\Omega = 1/\pi$, then:
\begin{equation}
w(h;y) \propto 2^{-K(y|h) \cdot \pi / \ln 2} \approx 2^{-4.53 \cdot K(y|h)}
\end{equation}
This represents \emph{more aggressive} complexity penalization than standard Solomonoff weightingâ€”a ``colder'' selection temperature.

Whether $\Omega = 1/\pi$ has deeper significance remains an open question, but it is mathematically distinct from Solomonoff correspondence.
\end{remark}

%==============================================================================
\section{What Each Framework Provides}
%==============================================================================

\subsection{What Solomonoff Provides}

\begin{enumerate}
    \item \textbf{Optimal prior:} $P(h) = 2^{-K(h)}$ over hypotheses (programs)
    \item \textbf{Optimal posterior:} $P(h|y) \propto 2^{-K(h)} \cdot 2^{-K(y|h)}$
    \item \textbf{Convergence guarantee:} Total prediction error bounded by $K(\mu) \ln 2$
\end{enumerate}

\subsection{What Solomonoff Does Not Provide}

\begin{enumerate}
    \item \textbf{When to act:} No decision rule for committing to a model
    \item \textbf{Computability:} $K(h)$ is uncomputable
    \item \textbf{Model change mechanism:} How one model replaces another
    \item \textbf{Parameter flexibility:} Prior is fixed by choice of universal machine
\end{enumerate}

\subsection{What CRR Adds}

\begin{enumerate}
    \item \textbf{Decision rule:} Switch when $\Sur_m - \Sur_{m'} > \Delta_{m,m'}$

    \item \textbf{Computable proxy:} Temporal surprisal accumulation approximates algorithmic complexity

    \item \textbf{Regeneration mechanism:} $R[\phi] = Z^{-1}\int \phi \cdot \exp(-\Sur/\Omega) \, d\tau$

    \item \textbf{Two tunable parameters:}
    \begin{itemize}
        \item $\Delta_{m,m'}$: Switching threshold (model-dependent)
        \item $\Omega$: Selection temperature (global)
    \end{itemize}
\end{enumerate}

%==============================================================================
\section{Compatibility Analysis}
%==============================================================================

\begin{theorem}[Compatibility]\label{thm:compatibility}
CRR and Solomonoff induction are mathematically compatible. CRR operationalizes aspects of Solomonoff for finite agents:

\begin{center}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Solomonoff} & \textbf{CRR} \\
\midrule
$K(y|h)$ (data complexity) & $\Sur_h(n)$ (accumulated surprisal) \\
$K(m') - K(m)$ (model complexity diff) & $\Delta_{m,m'}$ (switching threshold) \\
Likelihood weighting $2^{-K(y|h)}$ & Regeneration $\exp(-\Sur/\Omega)$ \\
Continuous mixture & Discrete rupture at threshold \\
\bottomrule
\end{tabular}
\end{center}
\end{theorem}

\begin{remark}[What CRR Does Not Capture]
CRR regeneration captures evidence weighting but not the Solomonoff prior $2^{-K(h)}$ directly. The prior enters CRR through:
\begin{itemize}
    \item The model set $\M$ (which models are considered)
    \item The prior $p(m)$ (implicit in $\Delta_{m,m'} = \log(p(m)/p(m'))$ interpretation)
\end{itemize}
\end{remark}

\begin{remark}[One-Sentence Summary]
CRR describes how bounded agents can approximate Bayesian evidence weighting through temporal dynamics and threshold-triggered model switching, using surprisal accumulation as a computable proxy for algorithmic complexity.
\end{remark}

%==============================================================================
\section{Comparison Tables}
%==============================================================================

\begin{table}[h!]
\centering
\caption{Parameter Roles}
\label{tab:parameters}
\begin{tabular}{@{}p{2.5cm}p{5cm}p{5cm}@{}}
\toprule
\textbf{Parameter} & \textbf{Role} & \textbf{Solomonoff Analogue} \\
\midrule
$\Delta_{m,m'}$ & Switching threshold; when to change models & $K(m') - K(m)$ (complexity overhead) \\
$\Omega$ & Temperature; how sharply to weight by fit & $1/\ln 2$ for exact correspondence \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h!]
\centering
\caption{What Each Framework Provides}
\label{tab:provides}
\begin{tabular}{@{}p{5cm}cc@{}}
\toprule
\textbf{Feature} & \textbf{Solomonoff} & \textbf{CRR} \\
\midrule
Prior over hypotheses $2^{-K(h)}$ & Yes & Indirect (via $p(m)$) \\
Likelihood weighting $2^{-K(y|h)}$ & Yes & Yes ($\exp(-\Sur/\Omega)$) \\
Optimal prediction guarantee & Yes & No \\
Decision rule (when to switch) & No & Yes ($\Sur_m - \Sur_{m'} > \Delta$) \\
Computable & No & Yes (for finite $\M$) \\
Tunable parameters & No & Yes ($\Delta$, $\Omega$) \\
\bottomrule
\end{tabular}
\end{table}

%==============================================================================
\section{Limitations and Open Questions}
%==============================================================================

\subsection{Limitations of CRR}

\begin{enumerate}
    \item \textbf{No universal prior:} CRR regeneration captures evidence weighting but not the Solomonoff prior directly

    \item \textbf{Model class restriction:} Requires specifying $\M$ in advance

    \item \textbf{Parameter dependence:} Results depend on $\Delta$ and $\Omega$, which must be determined

    \item \textbf{No convergence guarantee:} Unlike Solomonoff, no proof that CRR converges to truth
\end{enumerate}

\subsection{Limitations of Solomonoff}

\begin{enumerate}
    \item \textbf{Incomputable:} Cannot be implemented

    \item \textbf{No decision rule:} Provides probabilities but not when to act

    \item \textbf{No temporal structure:} Does not address discrete change
\end{enumerate}

\subsection{Open Questions}

\begin{enumerate}
    \item Can CRR be extended to include a proper Solomonoff-like prior term?

    \item Is there a principled derivation of $\Omega$ (independent of Solomonoff matching)?

    \item How should $\Delta_{m,m'}$ be set in practice?

    \item Does CRR with appropriate parameters converge to Solomonoff as $|\M| \to \infty$?
\end{enumerate}

%==============================================================================
\section{Conclusion}
%==============================================================================

This analysis establishes:

\begin{enumerate}
    \item \textbf{Partial correspondence:} CRR surprisal corresponds to $K(y|h)$; regeneration implements likelihood/evidence weighting $2^{-K(y|h)}$, not the full Solomonoff posterior

    \item \textbf{Parameter separation:} The switching threshold $\Delta$ and temperature $\Omega$ serve distinct roles and should not be conflated

    \item \textbf{Sign consistency:} Regeneration must use $\exp(-\Sur/\Omega)$ (negative exponent) for mathematical consistency

    \item \textbf{Complementarity:} Solomonoff specifies \emph{what} is optimal; CRR specifies \emph{when} to switch and \emph{how} to weight evidence
\end{enumerate}

The relationship: \emph{CRR implements the evidence-weighting component of Bayesian inference with explicit decision rules, using surprisal accumulation as a computable proxy for algorithmic complexity.}

%==============================================================================
\begin{thebibliography}{99}

\bibitem{solomonoff1964}
Solomonoff, R.J. (1964). A formal theory of inductive inference. \textit{Information and Control}, 7(1), 1-22.

\bibitem{kolmogorov1965}
Kolmogorov, A.N. (1965). Three approaches to the quantitative definition of information. \textit{Problems of Information Transmission}, 1(1), 1-7.

\bibitem{levin1974}
Levin, L.A. (1974). Laws of information conservation. \textit{Problems of Information Transmission}, 10(3), 206-210.

\bibitem{rissanen1978}
Rissanen, J. (1978). Modeling by shortest data description. \textit{Automatica}, 14(5), 465-471.

\bibitem{wald1947}
Wald, A. (1947). \textit{Sequential Analysis}. John Wiley \& Sons.

\bibitem{hutter2005}
Hutter, M. (2005). \textit{Universal Artificial Intelligence}. Springer.

\bibitem{grunwald2007}
Gr\"{u}nwald, P.D. (2007). \textit{The Minimum Description Length Principle}. MIT Press.

\end{thebibliography}

%==============================================================================
\appendix
\section{Technical Notes}
%==============================================================================

\subsection{Levin's Coding Theorem}

\begin{theorem}[Levin]
For any computable probability distribution $P$ and string $x$:
\begin{equation}
-\log P(x) = K(x) + O(K(P))
\end{equation}
\end{theorem}

\subsection{Chain Rule for Kolmogorov Complexity}

\begin{theorem}[Chain Rule]
\begin{equation}
K(x, y) = K(x) + K(y|x^*) + O(\log K(x,y))
\end{equation}
where $x^*$ is the shortest program for $x$.
\end{theorem}

\subsection{Why the Negative Sign Matters}

In the original CRR formulation, if coherence $C$ is defined as accumulated prediction error (a cost), then weighting by $\exp(+C/\Omega)$ would favor \emph{worse}-fitting states.

The correct form is either:
\begin{itemize}
    \item Define $C$ as negative surprisal (higher = better fit), weight by $\exp(+C/\Omega)$
    \item Define $\Sur$ as surprisal/cost (higher = worse fit), weight by $\exp(-\Sur/\Omega)$
\end{itemize}

This document uses the second convention for clarity.

\end{document}
